# HyperMorphic-Transformer
HyperMorphic Transformer

# Biomorphic Transformer â€” HyperMorphic Optimizer Suite

**One-file, reproducible benchmark** for evaluating a **Biomorphic (Î¦/Î¨ + Îµâ‚• + trust-blend)** optimizer on a
calibrationâ€‘sensitive **sequence modeling** task with a tiny **causal Transformer**.

> TL;DR: This repo shows how a biologicallyâ€‘inspired optimizer (HyperMorphic, â€œHMâ€) improves **loss, calibration (ECE)**, and **trajectory stability** under noise & shiftâ€”using clean ablations, stats, sensitivity scans, and optional plots.

---

## âœ¨ Whatâ€™s inside

- **`biomorphic_transformer.py`** â€” the entire project in **one script**:
  - **Model**: small causal **Transformer** (2 layers by default).
  - **Task**: synthetic nextâ€‘token prediction with **regime switch** + **domainâ€‘shifted validation** (great for calibration stressâ€‘tests).
  - **Optimizers**:
    - Baselines: **Adam**, **AdamClip**.
    - **HyperMorphic (HM)** variants:
      - `HM_Static` â€” Î¦ + Î¨ + **Îµâ‚•** + trustâ€‘blend (fixed params).
      - `HM_Bio` â€” as above **with biomorphic controller** (smooth schedules for Îµâ‚•, base \(b\), Î±, Î², Î»).
      - Ablations: `HM_no_eps` (no Îµâ‚•), `HM_phi_only` (Î¦ only), `HM_psi_only` (Î¨ only).
  - **Metrics**: validation **loss**, **ECE** (Expected Calibration Error), **update alignment** (â†‘ good), **flipâ€‘rate** (â†“ good), **deltaâ€‘norm variance** (â†“ good).
  - **Statistics**: **95% CI**, **paired tâ€‘test** and **Wilcoxon** vs `HM_Bio`.
  - **Sensitivity scans**: sweep **(Îµâ‚•, b, Î±, Î², Î»)**; optional plots.

---

## ğŸ§  Core idea (Biological coherence)

The HyperMorphic pipeline maps raw gradients through two bounded transforms and a trustâ€‘blend:

- **Î¦ (logâ€‘compression)**: \( \Phi_{\varepsilon,b}(x)=\mathrm{sgn}(x)\,\log_b\!\big(1+\frac{|x|}{\varepsilon}\big) \)  
  Contracts large/lowâ€‘SNR signals; controlled by **Îµâ‚•** and base **b**.

- **Î¨ (bounded fold)**: \( \Psi_{\alpha,\beta}(x)=\sin\!\big(\beta\,\tanh(\alpha x)\big) \)  
  Binds updates inside \([-1,1]\) with slope control via **Î±, Î²**.

- **Trustâ€‘blend (Î»)**: \( d_t \propto \lambda\,\hat u + (1-\lambda)\,d_{t-1} \)  
  Caps perâ€‘step turning, reducing flipâ€‘rate & variance.

- **Biomorphic controller**: Smoothly modulates \((\varepsilon,b,\alpha,\beta,\lambda)\) over time; acts like a **homeostatic governor**â€”more contractive in noisy phases, looser in stable phases.

**Why it helps:** Under noise and shift, HM acts like **adaptive temperature scaling** on logits (shrinks overconfidence) while **bounding curvature** of the optimization path â†’ **ECE â†“ with stable loss**.

---

## âš™ï¸ Installation

```bash
# Python 3.9+ recommended
pip install torch pandas matplotlib
# (pandas/matplotlib are optional but used for CSVs and plots)
```

> CPU is fine. GPU is optional. No external datasets required.

---

## ğŸš€ Quick start

Run baselines + HM variants with **10 seeds**, compute **95% CIs** + **paired tests**, and write CSVs:

```bash
python biomorphic_transformer.py \
  --seeds 10 --epochs 3 --device cpu \
  --opts Adam AdamClip HM_Static HM_Bio HM_no_eps HM_phi_only HM_psi_only \
  --out_prefix HM_TinyTX
```

Outputs (CSV):
- `HM_TinyTX_PerSeed.csv` â€” perâ€‘seed metrics
- `HM_TinyTX_Summary_95CI.csv` â€” mean Â± 95% CI for each metric
- `HM_TinyTX_Stats.csv` â€” paired t / Wilcoxon vs **HM_Bio** on loss/ECE/alignment

---

## ğŸ” Sensitivity scans & plots

Scan parameter ranges and build loss/ECE curves with 95% CIs:

```bash
python biomorphic_transformer.py \
  --seeds 6 --epochs 2 --device cpu \
  --do_scans --scan_params eps_h base_b alpha beta lam \
  --plots \
  --out_prefix HM_TinyTX
```

Additional figures (if `--plots`):
- `HM_TinyTX_loss_by_opt.png` â€” mean val loss by optimizer (+CI)
- `HM_TinyTX_ece_by_opt.png` â€” mean ECE by optimizer (+CI)
- `HM_TinyTX_loss_vs_<param>.png`, `HM_TinyTX_ece_vs_<param>.png` â€” sensitivity curves (+CI)

---

## ğŸ“ Metrics (how theyâ€™re computed)

- **Loss** â€” average negative logâ€‘likelihood on all but last timestep.
- **ECE** â€” 10â€‘bin reliability gap between accuracy & confidence. Lower is **better**.
- **Update alignment** â€” cosine similarity between successive parameter deltas (â†‘ smoother path).
- **Flipâ€‘rate** â€” fraction of coordinates with sign flips between successive updates (â†“ fewer reversals).
- **Deltaâ€‘norm variance** â€” variance of \(\|\Delta\theta\|\) over steps (â†“ stabler magnitudes).

---

## ğŸ§ª Ablations & baselines

| Label         | Description                                      |
|---------------|--------------------------------------------------|
| `Adam`        | Standard Adam                                    |
| `AdamClip`    | Adam + global grad clipping (1.0)                |
| `HM_Static`   | Î¦ + Î¨ + Îµâ‚• + trustâ€‘blend (fixed params)          |
| `HM_Bio`      | HM + **biomorphic controller** (adaptive params) |
| `HM_no_eps`   | HM without Îµâ‚• floor                              |
| `HM_phi_only` | HM with Î¦ only (no Î¨ fold)                       |
| `HM_psi_only` | HM with Î¨ only (no Î¦ compression)                |

**What to expect (qualitative):**
- **HM_Bio** typically improves **loss** and **ECE** vs baselines and `HM_Static`.
- Removing **Îµâ‚•** tends to **worsen ECE** (overconfidence returns).
- **Î¦ only / Î¨ only** underperform **Î¦+Î¨** (the combo is the point).
- Midâ€‘high **Î»** reduces flipâ€‘rate and increases alignment (bounded curvature).

> Your exact numbers will vary by seed; use the included **95% CIs** and **paired tests** for rigor.

---

## ğŸ§© Command reference (selected)

```bash
# change model size
--d_model 64 --d_ff 128 --n_heads 4 --n_layers 2

# dataset size
--n_train 2000 --n_val 1000 --seq_len 32 --vocab 16

# HM hyperparameters
--lr_hm 2.5e-3 --eps_h 2e-3 --base_b 2.0 --alpha 1.1 --beta 0.9 --lam 0.75

# choose which optimizers to run
--opts Adam HM_Bio

# plotting
--plots
```

Run `python biomorphic_transformer.py -h` for full CLI.

---

## ğŸ“Š Interpreting results

1. Open `*_Summary_95CI.csv` â€” compare **loss** and **ECE** means + **95% CI** across optimizers.  
2. Check `*_Stats.csv` â€” **paired t / Wilcoxon** vs `HM_Bio` (lower pâ€‘values â†’ stronger evidence).  
3. Inspect stability metrics:
   - **alignment â†‘**, **flipâ€‘rate â†“**, **deltaâ€‘norm variance â†“** â†’ smoother learning dynamics.
4. If running scans, find the **Îµâ‚•** region where **ECE** dips (calibration sweetâ€‘spot) without hurting loss.

---

## ğŸ§ª Reproducibility notes

- **Deterministic**: seeds are set for Python, NumPy, and PyTorch; pass `--seeds N --base_seed S` for paired experiments.
- **CPU/GPU**: results should be similar on CPU; GPU nondeterminism can add tiny noise unless fully pinned.

---

## ğŸ§  Theory cheatâ€‘sheet (why HM helps)

- **Contraction:** Î¦âˆ˜Î¨ has global Lipschitz constant \(L \le \alpha\beta/(\varepsilon\ln b)\) â†’ more contraction when Îµâ‚• is larger or \(b\) is higher.  
- **Curvature bound:** trustâ€‘blend caps perâ€‘step turning by \( \arcsin(\lambda) \) â†’ fewer sign flips & less variance.  
- **Calibration link:** bounded, adaptive shrinkage of logit steps behaves like **temperature scaling** where it matters â†’ **ECE â†“** with decisions often unchanged.

For a full writeâ€‘up (statements & proof sketches), see the â€œBiological coherenceâ€ notes in the project discussion.

---

## â“ FAQ

- **Do I need a dataset?** No â€” the script generates a synthetic regimeâ€‘switching corpus.
- **Can I plug HM into my model?** Yes â€” the HM optimizer is a dropâ€‘in `torch.optim.Optimizer`â€‘style class.
- **Why ECE?** Because deployment safety needs calibrated probabilities, not just low loss.

---

## ğŸ“„ License

MIT. See your repositoryâ€™s `LICENSE` file (recommended).

---

## ğŸ™Œ Citation

If you use this in academic or industrial work, please cite:

```
@software{BiomorphicTransformer2025,
  title   = {Biomorphic Transformer: HyperMorphic Optimizer Suite},
  author  = {Gerrard, Shaun Paul and Collaborators},
  year    = {2025},
  url     = {https://github.com/<your-org>/biomorphic-transformer}
}
```

---

## ğŸ¤ Acknowledgments

Thanks to the communities exploring **calibration**, **robust optimization**, and **bioâ€‘inspired control**â€”this project stands on many shoulders.

---

**Go build.** ğŸ”§ **Go measure.** ğŸ“ˆ **Go calibrate.** ğŸ¯
